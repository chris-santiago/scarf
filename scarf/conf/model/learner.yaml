# Defining the optimizer as a group default allows CLI override, e.g.
# python train.py "optimizer@model.optimizer=sgd"
# or via config "override scheduler@model.scheduler: cyclic"
# See https://stackoverflow.com/questions/71438040/overwriting-hydra-configuration-groups-from-cli/71439510#71439510
defaults:
  - /optimizer@optimizer: adam
  - /scheduler@scheduler: plateau

name: scarf-learner

nn:
  _target_: scarf.models.scarf.SCARFLearner
  encoder_ckpt: outputs/scarf-encoder/train_self/2024-03-05/21-45-12/checkpoints/epoch=91-step=1288.ckpt
  dim_hidden: 256
  n_layers: 2
  num_classes: 7
  loss_func:
    _target_: torch.nn.CrossEntropyLoss
  score_func:
    _target_: torchmetrics.Accuracy
    task: multiclass
    num_classes: ${model.nn.num_classes}