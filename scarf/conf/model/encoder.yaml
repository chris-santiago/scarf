# Defining the optimizer as a group default allows CLI override, e.g.
# python train.py "optimizer@model.optimizer=sgd"
# or via config "override scheduler@model.scheduler: cyclic"
# See https://stackoverflow.com/questions/71438040/overwriting-hydra-configuration-groups-from-cli/71439510#71439510
defaults:
  - /optimizer@optimizer: adam
  - /scheduler@scheduler: plateau

name: scarf-encoder

nn:
  _target_: scarf.models.scarf.SCARFEncoder
  dim_hidden: 256
  n_encoder_layers: 4
  n_projection_layers: 2
  p_mask: 0.7
  loss_func:
    _target_: pytorch_metric_learning.losses.SelfSupervisedLoss
    loss:
      _target_: pytorch_metric_learning.losses.NTXentLoss
      temperature: 1.0
  batch_norm: True
